{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f738b23",
   "metadata": {},
   "source": [
    "## Writing to Files in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30370eac",
   "metadata": {},
   "source": [
    "## Appending a file   'a'\n",
    "\n",
    "Appending a file adds a line to the end of a file without overwriting the whole file and completely deleting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1ca287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a file- appending the words \"end of assessment\" to a file\n",
    "\n",
    "with open(r'C:\\Users\\Hp\\Downloads\\sql_assessment_watu.txt','a') as watu_assessment:\n",
    "    watu_assessment.write(\"\\nEnd of Assessment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9974c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "-- TABLES STRUCTURE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NOTE: these are just extracts from bigger tables, to show their structure\n",
      "\n",
      "  # table name: client \n",
      "\n",
      "|client_id|first_name|middle_name|last_name|date_of_birth|\n",
      "|---------|----------|-----------|---------|-------------|\n",
      "|33       |Susan     |Mapenzi    |Marigu   |1974-06-11   |\n",
      "|35       |Paul      |           |Pogba    |1993-03-15   |\n",
      "|36       |Hafsa     |Wangui     |Munga    |1987-05-07   |\n",
      "|37       |Everlyne  |           |Maten'ge |1973-02-27   |\n",
      "|38       |Barack    |           |Obama    |1961-08-04   |\n",
      "|39       |Prudence  |Salim      |Okeyo    |1985-02-16   |\n",
      "|40       |Rosemary  |Pauline    |Kinyua   |1977-01-27   |\n",
      "|42       |Elizabeth |           |Mbaji    |1975-10-03   |\n",
      "|43       |Johny     |Paul       |Orengo   |1971-07-29   |\n",
      "|44       |Merceline |Lucy       |Njenga   |1982-04-21   |\n",
      "\n",
      "  # table name: loan\n",
      "  \n",
      "|loan_id|client_id|vehicle_id|principal_amount|submitted_on_date|\n",
      "|-------|---------|----------|----------------|-----------------|\n",
      "|75676  |40784    |24        |106500          |2019-05-02       |\n",
      "|75659  |40760    |26        |108400          |2020-12-05       |\n",
      "|75685  |40807    |27        |101500          |2019-05-02       |\n",
      "|75657  |40796    |28        |271482          |2019-06-21       |\n",
      "|75662  |40803    |29        |114400          |2019-05-02       |\n",
      "|75660  |40737    |30        |95300           |2019-05-02       |\n",
      "|75656  |40815    |31        |78500           |2019-05-02       |\n",
      "|75666  |40834    |32        |111800          |2019-05-02       |\n",
      "|75658  |40811    |33        |107050          |2019-05-02       |\n",
      "|75663  |40840    |34        |101800          |2019-05-02       |\n",
      "\n",
      "  # table name: vehicle\n",
      " \n",
      "|vehicle_id|make    |model_name      |\n",
      "|----------|--------|----------------|\n",
      "|24        |Haojin  |HJ 150CC-11A    |\n",
      "|26        |Honda   |Ace CB 125CC ES |\n",
      "|27        |TVS     |HLX 125CC ES    |\n",
      "|29        |TVS     |HLX 150CC X     |\n",
      "|30        |TVS     |HLX 100CC KS    |\n",
      "|31        |Haojin  |HJ 125CC-A      |\n",
      "|32        |Boxer   |BM 150CC (4)    |   \n",
      "|33        |Ferrari |Enzo 6000CC     |\n",
      "|34        |Boxer   |BM 150cc-2      |\n",
      "|35        |Boxer   |BM 150cc-3      |\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "-- QUESTIONS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Using the tables above, please write the SQL code that would answer each of the following questions (write the code below each of them).\n",
      "\n",
      "1. -- Select all the clients called Paul in first_name or middle_name and who are more than 25 years old.\n",
      "   -- In the results, create a column with the client's age in years. \n",
      "   -- Order them from older to younger.\n",
      "    \n",
      "2. -- Add a column to the table from question (1) that contains the number of loans each customer made.\n",
      "   -- If there is no loan, this column should show 0.\n",
      "\n",
      "3. -- Select the 100cc, 125cc and 150cc bikes from the vehicle table.\n",
      "   -- Add an engine_size column to the output (that contains the engine size).\n",
      "\n",
      "4. -- Calculate the total principal_amount per client full name (one column that includes all the names for each client) and per vehicle make.\n",
      "\n",
      "5. -- Select the loan table and add an extra column that shows the chronological loan order for each client based on the submitted_on_date column: \n",
      "   -- 1 if it's the client's first sale, 2 if it's the client's second sale etc.\n",
      "   -- Call it loan_order\n",
      "End of Assessment\n",
      "End of Assessment\n"
     ]
    }
   ],
   "source": [
    "# reading the file to check if the words \"end of assessment\" have been appended\n",
    "\n",
    "with open(r'C:\\Users\\Hp\\Downloads\\sql_assessment_watu.txt','r') as watu_test:\n",
    "    watu_data = watu_test.read()\n",
    "print(watu_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56a4ff",
   "metadata": {},
   "source": [
    "## Writing a file   'w'\n",
    "\n",
    "Opening a file using the argument 'w' for write-mode completely overwrites the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91928a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using write-mode 'w' to write to a file in python\n",
    "\n",
    "with open(r'C:\\Users\\Hp\\Downloads\\sql_assessment_watu1.txt', 'w') as watu1:\n",
    "    watu1.write(\"File overwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccefc822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File overwritten\n"
     ]
    }
   ],
   "source": [
    "# reading the file to check content of the newly written file\n",
    "\n",
    "with open(r'C:\\Users\\Hp\\Downloads\\sql_assessment_watu1.txt', 'r') as watu1_assessment:\n",
    "    watu1_data = watu1_assessment.read()\n",
    "print(watu1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f152e3",
   "metadata": {},
   "source": [
    "## Writing a CSV file in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fecaa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product', 'ID', 'Product Name', 'Current Price', 'Number in stock', 'Customer Rating(Stars)', 'Return Rate']\n",
      "['CSw2104', 'Cotton', 'Sweater', '100', '95', '4.1', '0.0023']\n",
      "['WSc6070', 'Wool', 'Scarf', '50', '362', '3.1', '0.017031456']\n",
      "['OCo8824', 'Oversized', 'Coat', '650', '173', '3.2', '0.016146829']\n",
      "['LBa2237', 'Leather', 'Bag', '1000', '246', '3.3', '0.012281759']\n",
      "['LWa7799', 'Leather', 'Wallet', '175', '105', '3.2', '0.02']\n",
      "['CWa1982', 'Chronograph', 'Watch', '350', '65', '4.9', '0.00014']\n",
      "['WGl6543', 'Winter', 'Gloves', '75', '171', '3.1', '0.023441955']\n",
      "['LSw9754', 'Light', 'Sweatshirt', '45', '230', '3.5', '0.012335504']\n",
      "['ST-3720', 'Spring', 'T-Shirt', '25', '145', '3.7', '0.00452009']\n",
      "['PSh9297', 'Polo', 'Shirt', '35', '25', '4.2', '0.007590577']\n",
      "['PSk2742', 'Pleated', 'Skirt', '50', '100', '4', '0.0125']\n",
      "['LDr4317', 'Long', 'Dress', '125', '373', '4.5', '0.0015']\n",
      "['SDr6674', 'Sweater', 'Dress', '215', '314', '3.8', '0.018899556']\n",
      "['BSh1337', 'Button', 'Shirt', '60', '199', '3.5', '0.020247351']\n",
      "['CBl7926', 'Cotton', 'Blouse', '75', '243', '3.75', '0.016219379']\n",
      "['LSn4457', 'Leather', 'Sneakers', '175', '95', '4.7', '0.00225']\n",
      "['FFl5119', 'Flip', 'Flops', '25', '250', '3.9', '0.007859143']\n"
     ]
    }
   ],
   "source": [
    "# Reading and parsing a csv file\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(r'C:\\Users\\Hp\\Desktop\\Data Engineering\\Dummy_Product.csv','r') as dummy_product:\n",
    "    dummy_data = csv.reader(dummy_product)\n",
    "    \n",
    "    for line in dummy_data:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f172ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the original csv file using the csv.reader method\n",
    "\n",
    "# then using write-mode to overwrite the original file and create a new csv file with ';' as the delimiter\n",
    "\n",
    "with open(r'C:\\Users\\Hp\\Desktop\\Data Engineering\\Dummy_Product.csv','r') as dummy_product:\n",
    "    dummy_data = csv.reader(dummy_product)\n",
    "    \n",
    "    with open('new_dummy_product.csv','w') as dummy_pd:\n",
    "        pd_data = csv.writer(dummy_pd, delimiter = ';')\n",
    "        \n",
    "        for line in dummy_data:\n",
    "            pd_data.writerow(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ddeb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product;ID;Product Name;Current Price;Number in stock;Customer Rating(Stars);Return Rate']\n",
      "[]\n",
      "['CSw2104;Cotton;Sweater;100;95;4.1;0.0023']\n",
      "[]\n",
      "['WSc6070;Wool;Scarf;50;362;3.1;0.017031456']\n",
      "[]\n",
      "['OCo8824;Oversized;Coat;650;173;3.2;0.016146829']\n",
      "[]\n",
      "['LBa2237;Leather;Bag;1000;246;3.3;0.012281759']\n",
      "[]\n",
      "['LWa7799;Leather;Wallet;175;105;3.2;0.02']\n",
      "[]\n",
      "['CWa1982;Chronograph;Watch;350;65;4.9;0.00014']\n",
      "[]\n",
      "['WGl6543;Winter;Gloves;75;171;3.1;0.023441955']\n",
      "[]\n",
      "['LSw9754;Light;Sweatshirt;45;230;3.5;0.012335504']\n",
      "[]\n",
      "['ST-3720;Spring;T-Shirt;25;145;3.7;0.00452009']\n",
      "[]\n",
      "['PSh9297;Polo;Shirt;35;25;4.2;0.007590577']\n",
      "[]\n",
      "['PSk2742;Pleated;Skirt;50;100;4;0.0125']\n",
      "[]\n",
      "['LDr4317;Long;Dress;125;373;4.5;0.0015']\n",
      "[]\n",
      "['SDr6674;Sweater;Dress;215;314;3.8;0.018899556']\n",
      "[]\n",
      "['BSh1337;Button;Shirt;60;199;3.5;0.020247351']\n",
      "[]\n",
      "['CBl7926;Cotton;Blouse;75;243;3.75;0.016219379']\n",
      "[]\n",
      "['LSn4457;Leather;Sneakers;175;95;4.7;0.00225']\n",
      "[]\n",
      "['FFl5119;Flip;Flops;25;250;3.9;0.007859143']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# reading the new file created after the original was overwritten\n",
    "\n",
    "with open('new_dummy_product.csv', 'r') as dum_dum:\n",
    "    dum_df = csv.reader(dum_dum)\n",
    "    \n",
    "    for line in dum_df:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9e5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new csv file and defining the fields we are going to be using\n",
    "\n",
    "with open('company.csv','w') as company_csv:\n",
    "    fields = [ 'userid','name', 'age']\n",
    "    company_data = csv.DictWriter(company_csv,fieldnames = fields)\n",
    "    \n",
    "    company_data.writeheader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c5b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userid', 'name', 'age']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# reading/parsing the created company.csv file\n",
    "\n",
    "with open('company.csv', 'r') as comp_csv:\n",
    "    comp_data = csv.reader(comp_csv)\n",
    "    \n",
    "    for line in comp_data:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef8ab15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001\n",
      "jackie\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "# Entering the first row of data in the newly created company.csv file\n",
    "\n",
    "with open('company.csv','a', newline = '') as company_csv:\n",
    "    tup1 = ('001', 'jackie', '83')\n",
    "    company_content = csv.writer(company_csv)\n",
    "    \n",
    "    company_content.writerow(tup1)\n",
    "    \n",
    "    with open('company.csv', 'r') as comp_csv:\n",
    "        comp_data = csv.reader(comp_csv)\n",
    "    \n",
    "        for line in tup1:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec76ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002\n",
      "jax\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# Entering the second row of data in the newly created company.csv file\n",
    "\n",
    "with open('company.csv', 'a', newline = '') as company_csv:\n",
    "    tup2 = ('002', 'jax', '36')\n",
    "    company_content = csv.writer(company_csv)\n",
    "    \n",
    "    company_content.writerow(tup2)\n",
    "    \n",
    "    for line in tup2:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6569d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userid', 'name', 'age']\n",
      "[]\n",
      "['001', 'jackie', '83']\n",
      "['002', 'jax', '36']\n"
     ]
    }
   ],
   "source": [
    "# reading and parsing the company.csv file using csv.reader()\n",
    "\n",
    "with open('company.csv', 'r') as comp_csv:\n",
    "    comp_data = csv.reader(comp_csv)\n",
    "    \n",
    "    for line in comp_data:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "270e47d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userid': '001', 'name': 'jackie', 'age': '83'}\n",
      "{'userid': '002', 'name': 'jax', 'age': '36'}\n"
     ]
    }
   ],
   "source": [
    "# reading and parsing the company.csv file using csv.DictReader()\n",
    "\n",
    "with open('company.csv', 'r') as comp_csv:\n",
    "    comp_data = csv.DictReader(comp_csv)\n",
    "    \n",
    "    for line in comp_data:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad1f81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userid': '001', 'name': 'jackie'}\n",
      "{'userid': '002', 'name': 'jax'}\n"
     ]
    }
   ],
   "source": [
    "# reading and parsing the first two fields (userid and name) and dropping the age field\n",
    "\n",
    "with open('company.csv', 'r') as comp_csv:\n",
    "    comp_data = csv.DictReader(comp_csv)\n",
    "    \n",
    "    for line in comp_data:\n",
    "        del(line['age'])\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e282c2",
   "metadata": {},
   "source": [
    "## Writing a Json file in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9a6b5",
   "metadata": {},
   "source": [
    "We can use the Json library to translate Python objects to Json. This is especially useful in instances where you're using a Python library to serve web pages, you would also be able to serve Json.\n",
    "\n",
    "Let's say we have a Python dictionary we want to save as a Json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bfe2625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eventid': 100100, 'date': 1985, 'university': 'UoN', 'reported': 'Yes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating my sample dictionary object\n",
    "\n",
    "my_dictionary = { 'eventid' : 100100,\n",
    "                  'date': 2005-9-11,  \n",
    "                  'university': 'UoN',\n",
    "                  'reported': 'Yes' }\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d5a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening a new json file (myoutput.json) using write-mode\n",
    "\n",
    "# then using the json.dump method to write to the file\n",
    "\n",
    "# json.dump() takes two arguments- first the data object, then the file object we want to save.\n",
    "\n",
    "import json \n",
    "\n",
    "with open('myoutput.json', 'w') as myoutput:\n",
    "    json.dump(my_dictionary, myoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b5c03da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100100\n"
     ]
    }
   ],
   "source": [
    "with open('myoutput.json', 'r') as myoutput:\n",
    "    output = json.load(myoutput)\n",
    "    \n",
    "print(output['eventid'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
